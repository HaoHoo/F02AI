{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始AI - 认知服务 - 语音\n",
    "\n",
    "> ··· 您可以访问 [https://github.com/HaoHoo/F02AI](https://github.com/HaoHoo/F02AI) 来获得 “从零开始AI” 系列全部的内容。··· <br>\n",
    ">`请以原样使用或转发，欢迎Issue，敬请PR；如果觉得有些意思，欢迎Fork，敬谢Star。` \n",
    "\n",
    "从零开始AI的有关介绍中，我们将使用Azure认知服务的标准REST API服务接口，尝试认知服务的一个类别——语音服务。\n",
    "\n",
    "按照Azure目前提供的语音服务分类，接下来我们可以尝试如下几种能力：\n",
    "* [文本转语音](#TTS)\n",
    "* [语音转文本](#STT)\n",
    "\n",
    "和其他认知服务一样，语音服务也是一种运行在Azure的服务。为了使用Speech API，您需要一个订阅密钥 (subscription key)。别担心，您可以在[这里](https://docs.microsoft.com/azure/cognitive-services/Computer-vision/Vision-API-How-to-Topics/HowToSubscribe)获取免费的订阅密钥。或者在[这里](https://azure.microsoft.com/zh-cn/try/cognitive-services)获取试用的订阅密钥。\n",
    "\n",
    "获取订阅密钥后，请记录密钥及分配的Azure服务区域。后面我们调用这些API时，必须提供这两种信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**关于播放声音的说明**\n",
    "\n",
    "在示例的原始代码中，通过认知服务会生成WAV声音文件，然后需要手动播放。为了能更好地和代码交互，我们会尝试让代码直接播放声音。这部分的代码对jupyter notebook来说，需要安装用于支持声音播放的的库。相对代码简单的播放声音的库可以选择 ‘playsound’。如果是在自己搭建的jupyter notebook环境中运行这个记事本，可以返回到文件列表的界面，在右上角找到‘新建’，然后选择‘终端’。在打开的终端中，使用如下命令安装需要的库：\n",
    "\n",
    "<code>pip install playsound</code>\n",
    "\n",
    "'playsound'能够跨平台实现非常简单的声音播放，代码简单到只需要两行\n",
    "```python\n",
    "from playsound import playsound\n",
    "playsound('/path/to/a/sound/file/you/want/to/play.mp3')\n",
    "```\n",
    "源码可以在Github上访问 [playsound](https://github.com/TaylorSMarks/playsound)。作者说明了不同系统下工作的机制。\n",
    "```\n",
    "On Windows, uses windll.winmm. WAVE and MP3 have been tested and are known to work. Other file formats may work as well.\n",
    "On OS X, uses AppKit.NSSound. WAVE and MP3 have been tested and are known to work. In general, anything QuickTime can play, playsound should be able to play, for OS X.\n",
    "On Linux, uses GStreamer. Known to work on Ubuntu 14.04 and ElementaryOS Loki. Support for the block argument is currently not implemented.｜\n",
    "```\n",
    "需要说明的是，由于这个库本身存在一些bug，我们可能需要自行做一点修复。\n",
    "\n",
    "*如果是Windows系统，建议安装之后，打开 playsound.py 找到如下代码并添加差异部分：\n",
    "```python\n",
    "if block:\n",
    "    sleep(float(durationInMS) / 1000.0)\n",
    "    winCommand('close', alias)\n",
    "```\n",
    "*如果是Mac OS系统，运行代码可能会看到需要AppKit之类的报错。安装XCode Command Line Tool并执行：\n",
    "\n",
    "<code>pip install pyobjc</code>\n",
    "\n",
    "解决这个问题。如果发现不能播放路径中包含空格的文件，可以打开 playsound.py 找到如下代码并添加差异部分：\n",
    "```python\n",
    "        sound = 'file://' + sound\n",
    "        sound = sound.replace(' ', '%20')\n",
    "    url   = NSURL.URLWithString_(sound)\n",
    "```\n",
    "如果您使用免费的[Azure Notebooks](https://notebooks.azure.com)在线环境运行这个笔记本，由于没有权限运行'pip'安装库文件，您可以按照代码中的提示注释掉或删除用于播放的代码，然后将生成的WAV文件使用其他方式进行播放。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文字到语音 <a name=\"TTS\"> </a>\n",
    "\n",
    "准备好了吗？让我们正式开始认知服务从文字到语音的尝试吧。首先需要引入必要的库，然后按照提示输入从免费认知服务试用、免费Azure试用或已有Azure订阅中Speech服务资源里提供的订阅密钥。密钥在我们使用REST API调用时，需要在用于验证的Header中提供，也被称为`Ocp-Apim-Subscription-Key`。\n",
    "\n",
    "实际上，以下的代码基本来自于微软的示例代码，您可以访问 [Azure Cognitive TTS Samples\n",
    "](https://docs.microsoft.com/samples/azure-samples/cognitive-speech-tts/azure-cognitive-tts-samples/)来查看简介及下载源代码。为了适合运行在笔记本中，并且直接与认知服务交互，代码做了修改调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, time\n",
    "from xml.etree import ElementTree\n",
    "#Delete next line if use Azure Notebooks\n",
    "from playsound import playsound\n",
    "\n",
    "subscription_key = input('Please input your Service Key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您如果还记得之前我们使用认知服务的经历的话，应该对服务区域 `Service Region` 有些印象。有关文本转语音服务的服务区域，可以查看对应的 [Speech-to-text REST API](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-speech-to-text) 文档中服务区域的介绍。服务区域也就是提供认知服务的区域，一般我们获得的订阅密钥，是和服务区域关联的。\n",
    "因此，需要在查看服务密钥的地方，确认对应的服务区域识别字符串，并按照以下代码提示输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_region = input('please input your Service Region:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于基于语音的认知服务，由于很多时候对声音的处理是相对异步的，所以每个服务调用的会话，需要生成一个`token`来进行验证和返回。获取这个`token`的方法非常简单，只需要使用POST方法向认知服务生成`token`的服务终结点URL提交一个含有订阅密钥头部header的请求即可。返回的字符串就是我们后续调用服务需要的`token`。为了看看`token`到底长什么样，我们可以将其显示出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_token_url = \"https://\"+service_region+\".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "headers = {\n",
    "            'Ocp-Apim-Subscription-Key': subscription_key\n",
    "        }\n",
    "response = requests.post(fetch_token_url, headers=headers)\n",
    "access_token = str(response.text)\n",
    "\n",
    "print(access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拿到了`token`，我们就可以尝试调用语音认知服务了。对于文本转语音的服务，有专门的服务终结点URL，我们需要按照REST API的要求，构造这个服务终结点的URL并发起REST API调用。\n",
    "\n",
    "与之前我们尝试过的计算机视觉等认知服务不同，提交一个文本转语音的请求时，并不是直接使用订阅密钥，而是使用由其生成的`token`。用法是在头部header中包含一个`Authorization`字段，内容是以`Bearer `开头并加上`token`的字段。\n",
    "\n",
    "让我们先尝试一个简单的服务调用：拿到支持的语音清单。通过这个API，我们可以得到在当前服务区域可供使用的各种语言以及语言对应的语音模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://\"+service_region+\".tts.speech.microsoft.com/\"\n",
    "path = 'cognitiveservices/voices/list'\n",
    "constructed_url = base_url + path\n",
    "headers = {\n",
    "            'Authorization': 'Bearer ' + access_token,\n",
    "        }\n",
    "response = requests.get(constructed_url, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    print(\"\\nAvailable voices: \\n\" + response.text)\n",
    "else:\n",
    "    print(\"\\nStatus code: \" + str(response.status_code) +  \\\n",
    "          \"\\nSomething went wrong. Check your subscription key and headers.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的是标准的JSON格式数据。我们可以找一下，对于简体中文 `zh-CN`，认知服务提供了慧慧 `Huihui`、康康 `Kangkang`和瑶瑶 `Yaoyao`。这几个标准语音有性别区分。注意看有个数据字段叫做 `ShortName`，后面我们选择文本转语音使用的标准语音类型，就可以通过这个名称来选择。除了标准语音，在部分区域例如`westus2`等，还支持神经语音标准，能够提供更加自然顺畅的语音，并且可以使用[语音合成标记语言](https://docs.microsoft.com/zh-cn/azure/cognitive-services/speech-service/speech-synthesis-markup#adjust-speaking-styles)来配置和调整神经语音。\n",
    "\n",
    "语音服务能支持的语言范围，可以在[语音服务的语言和语音支持](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support#speech-to-text)这一文档中查看。\n",
    "\n",
    "返回的数据中还有一个参数`SampleRateHertz`采样率，这与认知服务支持的音频格式有关，我们将在POST代码部分时候介绍支持的音频格式。\n",
    "\n",
    "接下来，我们开始为文本转语音做些准备工作。首先做一个时间戳，提供给WAV文件生成时用来产生文件名。然后，我们需要按照提示，输入用于转变成语音的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "ttsstr = input(\"What would you like to convert to speech: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们生成一个用于给WAV文件命名的时间戳。\n",
    "\n",
    "然后，我们按照提示输入需要进行语音转换的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = \"https://\"+service_region+\".tts.speech.microsoft.com/\"\n",
    "path = 'cognitiveservices/v1'\n",
    "constructed_url = base_url + path\n",
    "headers = {\n",
    "        'Authorization': 'Bearer ' + access_token,\n",
    "        'Content-Type': 'application/ssml+xml',\n",
    "        'X-Microsoft-OutputFormat': 'riff-24khz-16bit-mono-pcm',\n",
    "        'User-Agent': 'YOUR_RESOURCE_NAME'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式POST一个文本转语音的请求时，API的服务终结点需要指定为特定的路径。同时，头部header除了之前使用的`token`之外，还需要说明后续`bady`的内容类型，以及需要返回的声音输出格式。\n",
    "\n",
    "语音认知服务目前能够支持的声音类型如下：\n",
    "\n",
    "```output\n",
    "raw-16khz-16bit-mono-pcm            raw-8khz-8bit-mono-mulaw\n",
    "riff-8khz-8bit-mono-alaw            riff-8khz-8bit-mono-mulaw\n",
    "riff-16khz-16bit-mono-pcm           audio-16khz-128kbitrate-mono-mp3\n",
    "audio-16khz-64kbitrate-mono-mp3     audio-16khz-32kbitrate-mono-mp3\n",
    "raw-24khz-16bit-mono-pcm            riff-24khz-16bit-mono-pcm\n",
    "audio-24khz-160kbitrate-mono-mp3    audio-24khz-96kbitrate-mono-mp3\n",
    "audio-24khz-48kbitrate-mono-mp3     ogg-24khz-16bit-mono-opus\n",
    "```\n",
    "\n",
    "此外，提交请求的`bady`部分需要满足[Speech Synthesis Markup Language (SSML)](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-synthesis-markup)的格式要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_body = ElementTree.Element('speak', version='1.0')\n",
    "# xml_body.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-us')\n",
    "xml_body.set('{http://www.w3.org/XML/1998/namespace}lang', 'zh-cn')\n",
    "voice = ElementTree.SubElement(xml_body, 'voice')\n",
    "# voice.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-US')\n",
    "voice.set('{http://www.w3.org/XML/1998/namespace}lang', 'zh-cn')\n",
    "# voice.set('name', 'en-US-Guy24kRUS') \n",
    "# Short name for 'Microsoft Server Speech Text to Speech Voice (en-US, Guy24KRUS)'\n",
    "voice.set('name', 'zh-CN-HuihuiRUS') \n",
    "# Short name for 'Microsoft Server Speech Text to Speech Voice (zh-CN, HuihuiRUS)'\n",
    "voice.text = ttsstr\n",
    "body = ElementTree.tostring(xml_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提供一个SSML格式`body`，有两部分。\n",
    "\n",
    "* 首先需要按照 w3.org 的规范，给出文档的描述。\n",
    "\n",
    "```XML\n",
    "<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"string\"></speak>\n",
    "```\n",
    "\n",
    "* 然后需要对具体使用的语音进行描述。在上述代码示例中，我们选择使用简体中文，并按照认知语音服务对于区域所支持的语音，选择标准语音的慧慧的声音`zh-CN-HuihuiRUS`。\n",
    "\n",
    "```XML\n",
    "    <voice name=\"zh-CN-HuihuiRUS\"></voice>\n",
    "```\n",
    "\n",
    "* 最后，我们传递之前输入的文本到这个XML文件中。最终的文档就形如：\n",
    "\n",
    "```XML\n",
    "<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xml:lang=\"string\">\n",
    "    <voice name=\"zh-CN-HuihuiRUS\">\n",
    "        认识服务是简单而神奇的人工智能体现。\n",
    "    </voice>\n",
    "</speak>\n",
    "```\n",
    "\n",
    "接下来的工作，就是把构造好的头部headers、包含SSML的bady提交到REST API的服务终结点地址了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(constructed_url, headers=headers, data=body)\n",
    "if response.status_code == 200:\n",
    "    with open('sample-' + timestr + '.wav', 'wb') as audio:\n",
    "        audio.write(response.content)\n",
    "        print(\"\\nStatus code: \" + str(response.status_code) + \"\\nYour TTS is ready for playback.\\n\")\n",
    "#Delete next line if use Azure Notebooks\n",
    "        playsound('sample-' + timestr + '.wav')\n",
    "else:\n",
    "    print(\"\\nStatus code: \" + str(response.status_code) + \"\\nSomething went wrong. Check your subscription key and headers.\\n\")\n",
    "    print(\"Reason: \" + str(response.reason) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如一切顺利，上述代码运行后，就能听到由您输入的文本生成的语音了。提交请求后，语音认知服务会返回声音的内容，将返回的数据写入一个WAV文件，就可以进行播放了。\n",
    "\n",
    "当jupyter notebook能够使用终端运行`pip`导入库时，就可以通过`playsound`直接播放这个文件了。\n",
    "\n",
    "播放结束后，可使用以下代码输入 y 或 n 来决定是否删除生成的WAV文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delfile = input(\"Delete the WAV file? (y/n):\")\n",
    "if delfile=='y':\n",
    "    os.remove('sample-' + timestr + '.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您选择了支持神经语音的服务区域，例如`westus2`，强烈建议您返回到构造SSML的代码块，将“慧慧”的标准语音对应的\"zh-CN-HuihuiRUS\"改为“晓悠“的神经语音\"zh-CN-XiaoyouNeural\"，然后好好的比较两者之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**关于录制声音的说明**\n",
    "\n",
    "前面我们使用的`playsound`用于简单的播放声音，但如果我们想通过麦克风录制声音然后让语音认知服务进行识别的时候，就需要一个支持麦克风输入的库了。所以我们来安装一个新的库:\n",
    "\n",
    "<code>pip install pyaudio</code>\n",
    "\n",
    "*在macOS上运行pip安装的时候，会报错提示找不到 portaudio.h 。解决这个问题并不复杂，使用如下 brew 命令行安装 portaudio 即可：\n",
    "\n",
    "<code>brew install portaudio</code>\n",
    "\n",
    "*在macOS Catalina上运行代码录音时，您也许会发现录制的WAV文件没有任何声音。估计这是由于系统偏好设置中，关于隐私的配置的问题。如果终端没有提示要访问麦克风，加入到麦克风的允许访问列表，运行代码时就不能访问麦克风。一个可以参考的做法是，安装一个命令行下访问麦克风的软件包，然后尝试触发终端的麦克风权限请求。\n",
    "\n",
    "<code>brew install sox</code>\n",
    "\n",
    "安装之后，就可以直接运行 sox 尝试让终端访问麦克风。还可以检查这个命令行生成的 test.wav 文件是否已经成功地录入了声音。\n",
    "\n",
    "<code>sox -d test.wav</code>\n",
    "\n",
    "按照网上的建议，还可以手动打开一个终端再运行上述步骤：\n",
    "\n",
    "<code>open /System/Applications/Utilities/Terminal.app/Contents/MacOS/Terminal</code>\n",
    "\n",
    "如果已经能够通过终端录制声音，我们就可以进入正式的代码尝试了。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语音到文字 <a name=\"STT\"> </a>\n",
    "\n",
    "关于语音到文字的智能识别，在微软的示例代码中却没找到 Python 的，所以参考[Speech-to-Text REST API](https://docs.microsoft.com/zh-cn/azure/cognitive-services/speech-service/rest-speech-to-text)文档中 C# 的示例代码，写出了以下 Python 的实现。\n",
    "\n",
    "实际上，如果我们顺着代码块一直运行到这里，是不需要再重新运行代码输入服务订阅密钥和服务区域的。为了避免使用超链接直接跳转到这里运行代码，缺少服务订阅密钥和服务区域导致报错，我们又一次使用代码提供了这些信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "subscription_key = input('Please input your Service Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_region = input('please input your Service Region:')\n",
    "\n",
    "fetch_token_url = \"https://\"+service_region+\".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "headers = {\n",
    "            'Ocp-Apim-Subscription-Key': subscription_key\n",
    "        }\n",
    "response = requests.post(fetch_token_url, headers=headers)\n",
    "access_token = str(response.text)\n",
    "\n",
    "print(access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了尽可能简化代码，我们可以首先用 `pyaudio` 来按照录制一个WAV文件，[Speech-to-Text REST API](https://docs.microsoft.com/zh-cn/azure/cognitive-services/speech-service/rest-speech-to-text)文档中介绍，通过REST API调用时，语音认知服务接受两种文件编码格式：\n",
    "\n",
    "| Format | Codec |Bit rate | Sample Rate |\n",
    "| - | - | - | - |\n",
    "| WAV | PCM | 256 kbps | 16 kHz, mono |\n",
    "| OGG | OPUS | 256 kpbs | 16 kHz, mono|\n",
    "\n",
    "因此，我们使用 `pyaudio` 录制语音以及保存为WAV文件时，应该按照这个标准设置好参数。录制将使用16 kHz的采样率、16 bit的比特率，单声道。为了减少等待时间，我们将录制的时间定义为 5 秒钟。如果需要，可以自行修改这个参数。\n",
    "我们使用一个名为 'test.wav' 的文件保存录制的语音。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"test.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了在包含多个支持录音的音频设备的计算机上运行这些代码，我们使用 `pyaudio` 来枚举这些设备，并让您选择使用哪一个。\n",
    "\n",
    "当看到 'Recording...' 的提示时，表示录音已经开始。您可以说一句简单的英文（因为后面的代码我们选择的语言是 `en-US`，如果您录制的是中文，请修改后续的代码）。大约 5 秒钟左右，'End record.' 会显示，表示录音结束。当然，录音时长可在前面的代码处修改。\n",
    "\n",
    "录制结束后，自带的 `wave` 库就会将生成的流写为一个WAV文件，供后续使用。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "print(\"----------------------record device list---------------------\")\n",
    "info = audio.get_host_api_info_by_index(0)\n",
    "numdevices = info.get('deviceCount')\n",
    "for i in range(0, numdevices):\n",
    "    if (audio.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "        print(\"Input Device id \", i, \" - \", audio.get_device_info_by_host_api_device_index(0, i).get('name'))\n",
    "\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "index = int(input(\"Please select device id:\"))\n",
    "print(\"recording via index \"+str(index))\n",
    "stream = audio.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording...\")\n",
    "frames = []\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "print(\"End record.\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是正式的语音认知服务的尝试了。我们会按照[Speech-to-Text REST API](https://docs.microsoft.com/zh-cn/azure/cognitive-services/speech-service/rest-speech-to-text)文档的要求，提交一个语音到文本的认知服务请求给 REST API。\n",
    "\n",
    "首先需要提供的是服务的终结点URL，形如：\n",
    "<code>https://&lt;REGION_IDENTIFIER&gt;.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1</code>\n",
    "\n",
    "我们将使用 `base_url` 加上 `path` 构造出完整的服务终结点URL。\n",
    "\n",
    "    ⚠️ 需要注意的是，必须将语言参数追加到 URL 以避免收到 4xx HTTP 错误。\n",
    "\n",
    "接着我们提供必要的参数给POST方法。重要的参数主要有这几个：\n",
    "    \n",
    "| 参数 | 说明 | 必需/可选 |\n",
    "| :-: | :- | :-: |\n",
    "| language | 标识所要识别的口语。 请参阅支持的语言。 | 必须 |\n",
    "| format | 指定结果格式。 接受的值为 simple 和 detailed。 简单结果包括 RecognitionStatus、DisplayText、Offset 和 Duration。 Detailed 响应包括显示文本的四种不同的表示形式。 默认设置为 simple。 | 可选 |\n",
    "\n",
    "一个标准的HTTP的POST方法还需要提供头部headers的参数。以下是主要的参数：\n",
    "\n",
    "| 标头 | 说明 | 必需/可选 |\n",
    "| :-: | :- | :- |\n",
    "| Ocp-Apim-Subscription-Key | 语音服务订阅密钥。 | 此标头或 Authorization 是必需的。|\n",
    "| Authorization | 前面带有单词 Bearer 的授权令牌。 有关详细信息，请参阅身份验证。 | 此标头或 Ocp-Apim-Subscription-Key 是必需的。|\n",
    "| Content-type | 描述所提供音频数据的格式和编解码器。 接受的值为 audio/wav; codecs=audio/pcm; samplerate=16000 和 audio/ogg; codecs=opus。| 必需 |\n",
    "| Accept | 如果提供此标头，则值必须是 application/json。 语音服务以 JSON 格式提供结果。 某些请求框架提供不兼容的默认值。 最好始终包含 Accept。| 可选，但建议提供。|\n",
    "\n",
    "与文本到语音不同，除了 token，语音到文本也支持订阅密钥的直接验证。两者二选一即可。\n",
    "    \n",
    "最后是POST这个请求最重要的部分，我们将打开之前录制的WAV文件，将其作为请求的body使用。并且对返回的响应进行阅读和分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://\"+service_region+\".stt.speech.microsoft.com/\"\n",
    "path = 'speech/recognition/conversation/cognitiveservices/v1'\n",
    "constructed_url = base_url + path\n",
    "params = {\n",
    "        'language': 'en-US',\n",
    "        'format': 'detailed'\n",
    "        }\n",
    "headers = {\n",
    "        'Authorization': 'Bearer ' + access_token,\n",
    "        'Content-Type': 'audio/wav; codecs=audio/pcm; samplerate=16000',\n",
    "        'Accept': 'application/json;text/xml'\n",
    "        }\n",
    "body = open('test.wav','rb').read()\n",
    "response = requests.post(constructed_url, params=params, headers=headers, data=body)\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(\"\\nStatus code: \" + str(response.status_code) + \"\\nSomething went wrong. Check your subscription key and headers.\\n\")\n",
    "    print(\"Reason: \" + str(response.reason) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一切正常，语音认知服务将返回JSON格式的数据。 如果前面参数我们选择的是 'simple' ，就能看到以下数据：\n",
    "\n",
    "| 参数 | 说明 |\n",
    "| :-: | :- |\n",
    "| RecognitionStatus | 状态，例如 Success 表示成功识别。|\n",
    "| DisplayText | 经过大小写转换、添加标点、执行反向文本规范化（将口头文本转换为短形式，例如，200 表示“two hundred”，或“Dr.Smith”表示“doctor smith”）和屏蔽亵渎内容之后的识别文本。 仅在成功时提供。|\n",
    "\n",
    "如果像上面代码中，选择了 'detailed' 参数，就能看到：\n",
    "\n",
    "| 参数 | 说明|\n",
    "| :-: | :- |\n",
    "| Confidence | 条目的置信度评分，从 0.0（完全不可信）到 1.0（完全可信）|\n",
    "| Lexical | 已识别文本的词法形式：识别的实际单词。|\n",
    "| ITN | 已识别文本的反向文本规范化（“规范”）形式，已应用电话号码、数字、缩写（“doctor smith”缩写为“dr smith”）和其他转换。|\n",
    "| MaskedITN | 可根据请求提供应用了亵渎内容屏蔽的 ITN 形式。|\n",
    "| Display | 已识别文本的显示形式，其中添加了标点符号和大小写形式。 此参数与将格式设置为 simple 时提供的 DisplayText 相同。|\n",
    "\n",
    "您在返回的数据中，找到了语音认知服务识别出的文本了吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，运行完毕后，您可以输入 y 或 n 来决定是否删除使用过的 test.wav 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delfile = input(\"Delete the WAV file? (y/n):\")\n",
    "if delfile=='y':\n",
    "    os.remove('test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "ecb75f60-0a83-4dec-a289-03241f755b2e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
